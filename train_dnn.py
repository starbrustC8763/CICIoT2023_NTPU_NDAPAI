# train_dnn_clean.py
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

# === åƒæ•¸ ===
INPUT_FILE = "processed_dataset.csv"
MODEL_FILE = "dnn_model_clean.h5"
ENCODER_FILE = "label_encoder.pkl"
SCALER_FILE = "scaler.pkl"
CONFUSION_IMG = "confusion_matrix_clean.png"

# === è®€å–è³‡æ–™ ===
print(f"ğŸ“¥ è¼‰å…¥è³‡æ–™ï¼š{INPUT_FILE}")
df = pd.read_csv(INPUT_FILE)

# === ç‰¹å¾µèˆ‡æ¨™ç±¤åˆ†é›¢ ===
X = df.drop("Label", axis=1)
y_raw = df["Label"]

# === LabelEncoderï¼ˆé‚„åŸé¡åˆ¥åç”¨ï¼‰ ===
le = LabelEncoder()
y = le.fit_transform(y_raw)
joblib.dump(le, ENCODER_FILE)

# === æ¨™æº–åŒ–ç‰¹å¾µï¼ˆæ­¤æ™‚ç‚ºä¿éšªèµ·è¦‹å†æ¬¡åšæ¨™æº–åŒ–ï¼‰===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
joblib.dump(scaler, SCALER_FILE)

# === One-hot ç·¨ç¢¼æ¨™ç±¤ ===
num_classes = len(le.classes_)
y_cat = to_categorical(y, num_classes)

# === åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›† ===
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_cat, test_size=0.3, stratify=y, random_state=42)

# === å»ºç«‹ DNN æ¨¡å‹ ===
model = Sequential([
    Dense(256, input_dim=X.shape[1], activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(num_classes, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# === è¨“ç·´æ¨¡å‹ ===
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
print("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...")
history = model.fit(X_train, y_train,
          epochs=20,
          batch_size=1024,
          validation_split=0.1,
          callbacks=[early_stop],
          verbose=2)

# === æ¨¡å‹è©•ä¼° ===
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nâœ… æ¸¬è©¦æº–ç¢ºç‡: {acc:.4f}")

# === é æ¸¬èˆ‡åˆ†é¡å ±å‘Š ===
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test, axis=1)

target_names = [str(cls) for cls in le.classes_]
print("\nğŸ“Š åˆ†é¡å ±å‘Šï¼š")
print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))

# === æ··æ·†çŸ©é™£å¯è¦–åŒ– ===
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(18, 14))
sns.heatmap(cm, annot=False, cmap='Blues', xticklabels=target_names, yticklabels=target_names)
plt.title("Confusion Matrix (Cleaned Data)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig(CONFUSION_IMG)
print(f"ğŸ–¼ï¸ æ··æ·†çŸ©é™£å„²å­˜ç‚º {CONFUSION_IMG}")

plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("loss_curve.png")
print("ğŸ“ˆ å·²å„²å­˜ loss æ›²ç·šåœ–ç‚º loss_curve.png")

# === å„²å­˜æ¨¡å‹ ===
model.save(MODEL_FILE)
print(f"âœ… æ¨¡å‹å·²å„²å­˜ç‚º {MODEL_FILE}")
